{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b89c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Image\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Dinov2Model, Dinov2PreTrainedModel\n",
    "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# Constants \n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "\n",
    "# Parameters\n",
    "dataset_name = \"ACPDS\" # ACPDS | PKLOT\n",
    "train_ratio = 0.6\n",
    "test_ratio = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384abb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "dataset_root = f\"{root}/{dataset_name}/{dataset_name}\"\n",
    "image_dir = os.path.join(dataset_root, \"images\")\n",
    "mask_dir = os.path.join(dataset_root, \"int_masks\")\n",
    "\n",
    "image_paths = sorted([\n",
    "    os.path.join(image_dir, f)\n",
    "    for f in os.listdir(image_dir)\n",
    "    if f.endswith(\".jpg\")\n",
    "])\n",
    "\n",
    "label_paths = sorted([\n",
    "    os.path.join(mask_dir, f.replace(\".jpg\", \".png\"))\n",
    "    for f in os.listdir(image_dir)\n",
    "    if f.endswith(\".jpg\")\n",
    "])\n",
    "\n",
    "combined = list(zip(image_paths, label_paths))\n",
    "random.seed(42)  \n",
    "random.shuffle(combined)\n",
    "\n",
    "train_size = int(len(combined)*train_ratio)\n",
    "test_size = int(len(combined)*test_ratio)\n",
    "val_size = len(combined) - train_size - test_size\n",
    "\n",
    "train_split = combined[:train_size]\n",
    "test_split = combined[train_size:train_size + test_size]\n",
    "val_split = combined[train_size + test_size:]\n",
    "\n",
    "# Unzip back\n",
    "train_imgs, train_masks = zip(*train_split)\n",
    "test_imgs, test_masks = zip(*test_split)\n",
    "val_imgs, val_masks = zip(*val_split)\n",
    "\n",
    "def create_dataset(image_paths, label_paths):\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"image\": image_paths,\n",
    "        \"label\": label_paths\n",
    "    })\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"label\", Image())\n",
    "    return dataset\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": create_dataset(train_imgs, train_masks),\n",
    "    \"test\": create_dataset(test_imgs, test_masks),\n",
    "    \"validation\": create_dataset(val_imgs, val_masks)\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e53cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "image = example[\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_map = example[\"label\"]\n",
    "segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc920d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_map = np.array(segmentation_map)\n",
    "segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"background\",\n",
    "    1: \"free\",\n",
    "    2: \"occupied\"\n",
    "}\n",
    "\n",
    "id2color = {\n",
    "    0: (255,255,255),\n",
    "    1: (0,255,0),\n",
    "    2: (255,0,0)\n",
    "}\n",
    "\n",
    "def visualize_map(image, segmentation_map):\n",
    "    color_seg = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) \n",
    "    for label, color in id2color.items():\n",
    "        color_seg[segmentation_map == label, :] = color\n",
    "\n",
    "    # Show image + mask\n",
    "    img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "visualize_map(image, segmentation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "  \n",
    "  def __init__(self, dataset, resize_size=(448, 448)):\n",
    "    self.dataset = dataset\n",
    "    self.resize_size = resize_size\n",
    "    self.normalize = transforms.Normalize(mean=ADE_MEAN, std=ADE_STD)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    og_image = np.array(item[\"image\"])\n",
    "    og_mask = np.array(item[\"label\"])\n",
    "\n",
    "    image = F.to_tensor(og_image)            # Converts to [C, H, W] and scales [0, 255] -> [0, 1]\n",
    "    mask = torch.from_numpy(og_mask).long()  # [H, W] with class ids\n",
    "\n",
    "    # Resize both image and mask\n",
    "    image = F.resize(image, self.resize_size, interpolation=InterpolationMode.BILINEAR)\n",
    "    mask = F.resize(mask.unsqueeze(0), self.resize_size, interpolation=InterpolationMode.NEAREST).squeeze(0)\n",
    "\n",
    "    # Normalize image\n",
    "    image = self.normalize(image)\n",
    "\n",
    "    return image, mask, og_image, og_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d541fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegmentationDataset(dataset[\"train\"])\n",
    "val_dataset = SegmentationDataset(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11159bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values, target, original_image, original_segmentation_map = train_dataset[3]\n",
    "print(pixel_values.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values_val, target_val, original_image_val, original_segmentation_map_val = val_dataset[3]\n",
    "print(pixel_values_val.shape)\n",
    "print(target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ef2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30aeb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "[id2label[id] for id in np.unique(original_segmentation_map).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8432452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(inputs):\n",
    "    batch = dict()\n",
    "    batch[\"pixel_values\"] = torch.stack([i[0] for i in inputs], dim=0)\n",
    "    batch[\"labels\"] = torch.stack([i[1] for i in inputs], dim=0)\n",
    "    batch[\"original_images\"] = [i[2] for i in inputs]\n",
    "    batch[\"original_segmentation_maps\"] = [i[3] for i in inputs]\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=3, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v,torch.Tensor):\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf62540",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"pixel_values\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"labels\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_image = (batch[\"pixel_values\"][0].numpy() * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "unnormalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[id2label[id] for id in torch.unique(batch[\"labels\"][0]).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_map(unnormalized_image, batch[\"labels\"][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad626902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.classifier = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "\n",
    "class Dinov2ForSemanticSegmentation(Dinov2PreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "\n",
    "    self.dinov2 = Dinov2Model(config)\n",
    "    self.classifier = LinearClassifier(config.hidden_size, 32, 32, config.num_labels)\n",
    "\n",
    "  def forward(self, pixel_values, output_hidden_states=False, output_attentions=False, labels=None):\n",
    "    # use frozen features\n",
    "    outputs = self.dinov2(pixel_values,\n",
    "                            output_hidden_states=output_hidden_states,\n",
    "                            output_attentions=output_attentions)\n",
    "    # get the patch embeddings - so we exclude the CLS token\n",
    "    patch_embeddings = outputs.last_hidden_state[:,1:,:]\n",
    "\n",
    "    # convert to logits and upsample to the size of the pixel values\n",
    "    logits = self.classifier(patch_embeddings)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      # important: we're going to use 0 here as ignore index instead of the default -100\n",
    "      # as we don't want the model to learn to predict background\n",
    "      loss_fct = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "      # print(\"Logits shape:\", logits.shape)\n",
    "      # print(\"Labels shape:\", labels.shape)\n",
    "      loss = loss_fct(logits, labels)\n",
    "\n",
    "    return SemanticSegmenterOutput(\n",
    "        loss=loss,\n",
    "        logits=logits,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Dinov2ForSemanticSegmentation.from_pretrained(\"facebook/dinov2-base\", id2label=id2label, num_labels=len(id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90beccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze dinov2\n",
    "for name, param in model.named_parameters():\n",
    "  if name.startswith(\"dinov2\"):\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c415b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "print(outputs.logits.shape)\n",
    "print(outputs.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_metric = evaluate.load(\"mean_iou\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c548a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# training hyperparameters\n",
    "# NOTE: I've just put some random ones here, not optimized at all\n",
    "# feel free to experiment, see also DINOv2 paper\n",
    "learning_rate = 1e-2\n",
    "epochs = 2\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# put model on GPU (set runtime to GPU in Google Colab)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# put model in training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "      pixel_values = batch[\"pixel_values\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      # forward pass\n",
    "      outputs = model(pixel_values, labels=labels)\n",
    "      loss = outputs.loss\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # evaluate\n",
    "      with torch.no_grad():\n",
    "        predicted = outputs.logits.argmax(dim=1)\n",
    "\n",
    "        # note that the metric expects predictions + labels as numpy arrays\n",
    "        iou_metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "\n",
    "      # let's print loss and metrics every 100 batches\n",
    "      if idx % 100 == 0:\n",
    "        metrics = iou_metric.compute(num_labels=len(id2label),\n",
    "                                ignore_index=0,\n",
    "                                reduce_labels=False,\n",
    "        )\n",
    "\n",
    "        print(\"Loss:\", loss.item())\n",
    "        print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "        print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f6bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "og_test_image = dataset[\"test\"][16][\"image\"]\n",
    "og_test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=ADE_MEAN, std=ADE_STD)\n",
    "\n",
    "test_image = F.to_tensor(og_test_image)\n",
    "test_image = F.resize(test_image, (448, 448), interpolation=InterpolationMode.BILINEAR)\n",
    "pixel_values = normalize(test_image).unsqueeze(0)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29700e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_logits = torch.nn.functional.interpolate(outputs.logits,\n",
    "                                                   size=og_test_image.size[::-1],\n",
    "                                                   mode=\"bilinear\", align_corners=False)\n",
    "predicted_map = upsampled_logits.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e98b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_map(og_test_image, predicted_map.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
